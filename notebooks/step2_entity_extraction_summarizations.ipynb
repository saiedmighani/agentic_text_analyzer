{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcab8dd6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8935f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd708912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/z7/043g_20d5x1578w9nh20hsjc0000gn/T/ipykernel_74317/4287715168.py\", line 11, in <module>\n",
      "    import spacy\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# NLP packages\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "# General packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4548fbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing tools initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sm/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/sm/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/sm/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/sm/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/sm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sm/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/sm/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize preprocessing tools\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"Preprocessing tools initialized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48e8ea",
   "metadata": {},
   "source": [
    "# Downloading Reuters corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd8c7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Reuters corpus...\n",
      "Loaded 10788 documents\n",
      "DataFrame shape: (10788, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_reuters_to_dataframe():\n",
    "    \"\"\"\n",
    "    Load all Reuters documents into a pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Get all file IDs\n",
    "    file_ids = reuters.fileids()\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    documents = []\n",
    "    doc_ids = []\n",
    "    categories_list = []\n",
    "    \n",
    "    # Extract data from each document\n",
    "    for file_id in file_ids:\n",
    "        try:\n",
    "            # Get document text\n",
    "            doc_text = reuters.raw(file_id)\n",
    "            \n",
    "            # Get categories for this document\n",
    "            doc_categories = reuters.categories(file_id)\n",
    "            \n",
    "            # Store data\n",
    "            documents.append(doc_text)\n",
    "            doc_ids.append(file_id)\n",
    "            categories_list.append(doc_categories)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'doc_id': doc_ids,\n",
    "        'text': documents,\n",
    "        'categories': categories_list\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading Reuters corpus...\")\n",
    "reuters_df = load_reuters_to_dataframe()\n",
    "print(f\"Loaded {len(reuters_df)} documents\")\n",
    "print(f\"DataFrame shape: {reuters_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbce3cc",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "855a1dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here we go again!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing special characters, extra whitespace, etc.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove HTML tags if any\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    # lower casing\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "sample_string = \"http. here    we go again!   \"\n",
    "\n",
    "clean_text(sample_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b31de4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>categories</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/14826</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
       "      <td>[trade]</td>\n",
       "      <td>asian exporters fear damage from u.s.-japan ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/14828</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
       "      <td>[grain]</td>\n",
       "      <td>china daily says vermin eat 7-12 pct grain sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/14829</td>\n",
       "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
       "      <td>[crude, nat-gas]</td>\n",
       "      <td>japan to revise long-term energy demand downwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/14832</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
       "      <td>[corn, grain, rice, rubber, sugar, tin, trade]</td>\n",
       "      <td>thai trade deficit widens in first quarter tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/14833</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...</td>\n",
       "      <td>[palm-oil, veg-oil]</td>\n",
       "      <td>indonesia sees cpo price rising sharply indone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_id                                               text  \\\n",
       "0  test/14826  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...   \n",
       "1  test/14828  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...   \n",
       "2  test/14829  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...   \n",
       "3  test/14832  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...   \n",
       "4  test/14833  INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...   \n",
       "\n",
       "                                       categories  \\\n",
       "0                                         [trade]   \n",
       "1                                         [grain]   \n",
       "2                                [crude, nat-gas]   \n",
       "3  [corn, grain, rice, rubber, sugar, tin, trade]   \n",
       "4                             [palm-oil, veg-oil]   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  asian exporters fear damage from u.s.-japan ri...  \n",
       "1  china daily says vermin eat 7-12 pct grain sto...  \n",
       "2  japan to revise long-term energy demand downwa...  \n",
       "3  thai trade deficit widens in first quarter tha...  \n",
       "4  indonesia sees cpo price rising sharply indone...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df['cleaned_text'] = reuters_df['text'].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")\n",
    "\n",
    "reuters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d2077a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\\n  Mounting trade friction between the\\n  U.S. And Japan has raised fears among many of Asia\\'s exporting\\n  nations that the row could inflict far-reaching economic\\n  damage, businessmen and officials said.\\n      They told Reuter correspondents in Asian capitals a U.S.\\n  Move against Japan might boost protectionist sentiment in the\\n  U.S. And lead to curbs on American imports of their products.\\n      But some exporters said that while the conflict would hurt\\n  them in the long-run, in the short-term Tokyo\\'s loss might be\\n  their gain.\\n      The U.S. Has said it will impose 300 mln dlrs of tariffs on\\n  imports of Japanese electronics goods on April 17, in\\n  retaliation for Japan\\'s alleged failure to stick to a pact not\\n  to sell semiconductors on world markets at below cost.\\n      Unofficial Japanese estimates put the impact of the tariffs\\n  at 10 billion dlrs and spokesmen for major electronics firms\\n  said they would virtually halt exports of products hit by the\\n  new taxes.\\n      \"We wouldn\\'t be able to do business,\" said a spokesman for\\n  leading Japanese electronics firm Matsushita Electric\\n  Industrial Co Ltd &lt;MC.T>.\\n      \"If the tariffs remain in place for any length of time\\n  beyond a few months it will mean the complete erosion of\\n  exports (of goods subject to tariffs) to the U.S.,\" said Tom\\n  Murtha, a stock analyst at the Tokyo office of broker &lt;James\\n  Capel and Co>.\\n      In Taiwan, businessmen and officials are also worried.\\n      \"We are aware of the seriousness of the U.S. Threat against\\n  Japan because it serves as a warning to us,\" said a senior\\n  Taiwanese trade official who asked not to be named.\\n      Taiwan had a trade trade surplus of 15.6 billion dlrs last\\n  year, 95 pct of it with the U.S.\\n      The surplus helped swell Taiwan\\'s foreign exchange reserves\\n  to 53 billion dlrs, among the world\\'s largest.\\n      \"We must quickly open our markets, remove trade barriers and\\n  cut import tariffs to allow imports of U.S. Products, if we\\n  want to defuse problems from possible U.S. Retaliation,\" said\\n  Paul Sheen, chairman of textile exporters &lt;Taiwan Safe Group>.\\n      A senior official of South Korea\\'s trade promotion\\n  association said the trade dispute between the U.S. And Japan\\n  might also lead to pressure on South Korea, whose chief exports\\n  are similar to those of Japan.\\n      Last year South Korea had a trade surplus of 7.1 billion\\n  dlrs with the U.S., Up from 4.9 billion dlrs in 1985.\\n      In Malaysia, trade officers and businessmen said tough\\n  curbs against Japan might allow hard-hit producers of\\n  semiconductors in third countries to expand their sales to the\\n  U.S.\\n      In Hong Kong, where newspapers have alleged Japan has been\\n  selling below-cost semiconductors, some electronics\\n  manufacturers share that view. But other businessmen said such\\n  a short-term commercial advantage would be outweighed by\\n  further U.S. Pressure to block imports.\\n      \"That is a very short-term view,\" said Lawrence Mills,\\n  director-general of the Federation of Hong Kong Industry.\\n      \"If the whole purpose is to prevent imports, one day it will\\n  be extended to other sources. Much more serious for Hong Kong\\n  is the disadvantage of action restraining trade,\" he said.\\n      The U.S. Last year was Hong Kong\\'s biggest export market,\\n  accounting for over 30 pct of domestically produced exports.\\n      The Australian government is awaiting the outcome of trade\\n  talks between the U.S. And Japan with interest and concern,\\n  Industry Minister John Button said in Canberra last Friday.\\n      \"This kind of deterioration in trade relations between two\\n  countries which are major trading partners of ours is a very\\n  serious matter,\" Button said.\\n      He said Australia\\'s concerns centred on coal and beef,\\n  Australia\\'s two largest exports to Japan and also significant\\n  U.S. Exports to that country.\\n      Meanwhile U.S.-Japanese diplomatic manoeuvres to solve the\\n  trade stand-off continue.\\n      Japan\\'s ruling Liberal Democratic Party yesterday outlined\\n  a package of economic measures to boost the Japanese economy.\\n      The measures proposed include a large supplementary budget\\n  and record public works spending in the first half of the\\n  financial year.\\n      They also call for stepped-up spending as an emergency\\n  measure to stimulate the economy despite Prime Minister\\n  Yasuhiro Nakasone\\'s avowed fiscal reform program.\\n      Deputy U.S. Trade Representative Michael Smith and Makoto\\n  Kuroda, Japan\\'s deputy minister of International Trade and\\n  Industry (MITI), are due to meet in Washington this week in an\\n  effort to end the dispute.\\n  \\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "736ea5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asian exporters fear damage from u.s.-japan rift mounting trade friction between the u.s. and japan has raised fears among many of asia\\'s exporting nations that the row could inflict far-reaching economic damage, businessmen and officials said. they told reuter correspondents in asian capitals a u.s. move against japan might boost protectionist sentiment in the u.s. and lead to curbs on american imports of their products. but some exporters said that while the conflict would hurt them in the long-run, in the short-term tokyo\\'s loss might be their gain. the u.s. has said it will impose 300 mln dlrs of tariffs on imports of japanese electronics goods on april 17, in retaliation for japan\\'s alleged failure to stick to a pact not to sell semiconductors on world markets at below cost. unofficial japanese estimates put the impact of the tariffs at 10 billion dlrs and spokesmen for major electronics firms said they would virtually halt exports of products hit by the new taxes. \"we wouldn\\'t be able to do business,\" said a spokesman for leading japanese electronics firm matsushita electric industrial co ltd &lt;mc.t>. \"if the tariffs remain in place for any length of time beyond a few months it will mean the complete erosion of exports (of goods subject to tariffs) to the u.s.,\" said tom murtha, a stock analyst at the tokyo office of broker &lt;james capel and co>. in taiwan, businessmen and officials are also worried. \"we are aware of the seriousness of the u.s. threat against japan because it serves as a warning to us,\" said a senior taiwanese trade official who asked not to be named. taiwan had a trade trade surplus of 15.6 billion dlrs last year, 95 pct of it with the u.s. the surplus helped swell taiwan\\'s foreign exchange reserves to 53 billion dlrs, among the world\\'s largest. \"we must quickly open our markets, remove trade barriers and cut import tariffs to allow imports of u.s. products, if we want to defuse problems from possible u.s. retaliation,\" said paul sheen, chairman of textile exporters &lt;taiwan safe group>. a senior official of south korea\\'s trade promotion association said the trade dispute between the u.s. and japan might also lead to pressure on south korea, whose chief exports are similar to those of japan. last year south korea had a trade surplus of 7.1 billion dlrs with the u.s., up from 4.9 billion dlrs in 1985. in malaysia, trade officers and businessmen said tough curbs against japan might allow hard-hit producers of semiconductors in third countries to expand their sales to the u.s. in hong kong, where newspapers have alleged japan has been selling below-cost semiconductors, some electronics manufacturers share that view. but other businessmen said such a short-term commercial advantage would be outweighed by further u.s. pressure to block imports. \"that is a very short-term view,\" said lawrence mills, director-general of the federation of hong kong industry. \"if the whole purpose is to prevent imports, one day it will be extended to other sources. much more serious for hong kong is the disadvantage of action restraining trade,\" he said. the u.s. last year was hong kong\\'s biggest export market, accounting for over 30 pct of domestically produced exports. the australian government is awaiting the outcome of trade talks between the u.s. and japan with interest and concern, industry minister john button said in canberra last friday. \"this kind of deterioration in trade relations between two countries which are major trading partners of ours is a very serious matter,\" button said. he said australia\\'s concerns centred on coal and beef, australia\\'s two largest exports to japan and also significant u.s. exports to that country. meanwhile u.s.-japanese diplomatic manoeuvres to solve the trade stand-off continue. japan\\'s ruling liberal democratic party yesterday outlined a package of economic measures to boost the japanese economy. the measures proposed include a large supplementary budget and record public works spending in the first half of the financial year. they also call for stepped-up spending as an emergency measure to stimulate the economy despite prime minister yasuhiro nakasone\\'s avowed fiscal reform program. deputy u.s. trade representative michael smith and makoto kuroda, japan\\'s deputy minister of international trade and industry (miti), are due to meet in washington this week in an effort to end the dispute.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df.iloc[0,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc723d",
   "metadata": {},
   "source": [
    "# rule-based entity extraction (inefficient way using part-of-sentence tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "866ceae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x: str, flag: str):\n",
    "    \"\"\"\n",
    "    Return list of words from sentence `x` that match POS family `flag`.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    try:\n",
    "        tokens = word_tokenize(x)\n",
    "        tagged = pos_tag(tokens)\n",
    "        for word, tag in tagged:\n",
    "            if tag in pos_family.get(flag, []):\n",
    "                result.append(word)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf0ce052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>categories</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>noun_list</th>\n",
       "      <th>adj_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/14826</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
       "      <td>[trade]</td>\n",
       "      <td>asian exporters fear damage from u.s.-japan ri...</td>\n",
       "      <td>[exporters, damage, rift, trade, friction, u.s...</td>\n",
       "      <td>[asian, u.s.-japan, many, far-reaching, econom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/14828</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
       "      <td>[grain]</td>\n",
       "      <td>china daily says vermin eat 7-12 pct grain sto...</td>\n",
       "      <td>[china, eat, pct, grain, stocks, survey, provi...</td>\n",
       "      <td>[vermin, 7-12, vermin, china, inadequate, bad,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/14829</td>\n",
       "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
       "      <td>[crude, nat-gas]</td>\n",
       "      <td>japan to revise long-term energy demand downwa...</td>\n",
       "      <td>[japan, energy, demand, ministry, trade, indus...</td>\n",
       "      <td>[long-term, international, long-term, japanese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/14832</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
       "      <td>[corn, grain, rice, rubber, sugar, tin, trade]</td>\n",
       "      <td>thai trade deficit widens in first quarter tha...</td>\n",
       "      <td>[thai, trade, deficit, quarter, thailand, trad...</td>\n",
       "      <td>[first, first, improved, pct, raw, semi-finish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/14833</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...</td>\n",
       "      <td>[palm-oil, veg-oil]</td>\n",
       "      <td>indonesia sees cpo price rising sharply indone...</td>\n",
       "      <td>[indonesia, sees, price, oil, cpo, prices, dlr...</td>\n",
       "      <td>[cpo, indonesia, crude, palm, better, european...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_id                                               text  \\\n",
       "0  test/14826  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...   \n",
       "1  test/14828  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...   \n",
       "2  test/14829  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...   \n",
       "3  test/14832  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...   \n",
       "4  test/14833  INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...   \n",
       "\n",
       "                                       categories  \\\n",
       "0                                         [trade]   \n",
       "1                                         [grain]   \n",
       "2                                [crude, nat-gas]   \n",
       "3  [corn, grain, rice, rubber, sugar, tin, trade]   \n",
       "4                             [palm-oil, veg-oil]   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  asian exporters fear damage from u.s.-japan ri...   \n",
       "1  china daily says vermin eat 7-12 pct grain sto...   \n",
       "2  japan to revise long-term energy demand downwa...   \n",
       "3  thai trade deficit widens in first quarter tha...   \n",
       "4  indonesia sees cpo price rising sharply indone...   \n",
       "\n",
       "                                           noun_list  \\\n",
       "0  [exporters, damage, rift, trade, friction, u.s...   \n",
       "1  [china, eat, pct, grain, stocks, survey, provi...   \n",
       "2  [japan, energy, demand, ministry, trade, indus...   \n",
       "3  [thai, trade, deficit, quarter, thailand, trad...   \n",
       "4  [indonesia, sees, price, oil, cpo, prices, dlr...   \n",
       "\n",
       "                                            adj_list  \n",
       "0  [asian, u.s.-japan, many, far-reaching, econom...  \n",
       "1  [vermin, 7-12, vermin, china, inadequate, bad,...  \n",
       "2  [long-term, international, long-term, japanese...  \n",
       "3  [first, first, improved, pct, raw, semi-finish...  \n",
       "4  [cpo, indonesia, crude, palm, better, european...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df['noun_list'] = reuters_df['cleaned_text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "reuters_df['adj_list'] = reuters_df['cleaned_text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "\n",
    "reuters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7348415",
   "metadata": {},
   "source": [
    "# A little more advanced extraction (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253906d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'the U.S. Department of Commerce', 'label': 'ORG', 'frequency': 1, 'length_tokens': 5, 'importance': 2.25}\n",
      "{'text': 'September 21, 2025', 'label': 'DATE', 'frequency': 1, 'length_tokens': 3, 'importance': 1.75}\n",
      "{'text': '$2 billion', 'label': 'MONEY', 'frequency': 1, 'length_tokens': 2, 'importance': 1.5}\n",
      "{'text': 'Tim Cook', 'label': 'PERSON', 'frequency': 1, 'length_tokens': 2, 'importance': 1.5}\n",
      "{'text': 'Apple', 'label': 'ORG', 'frequency': 1, 'length_tokens': 1, 'importance': 1.25}\n",
      "{'text': 'Austin', 'label': 'GPE', 'frequency': 1, 'length_tokens': 1, 'importance': 1.25}\n",
      "{'text': 'October', 'label': 'DATE', 'frequency': 1, 'length_tokens': 1, 'importance': 1.25}\n",
      "{'text': 'Texas', 'label': 'GPE', 'frequency': 1, 'length_tokens': 1, 'importance': 1.25}\n",
      "\n",
      "By label: {'ORG': ['the U.S. Department of Commerce', 'Apple'], 'DATE': ['September 21, 2025', 'October'], 'MONEY': ['$2 billion'], 'PERSON': ['Tim Cook'], 'GPE': ['Austin', 'Texas']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Most important labels\n",
    "IMPORTANT_LABELS = [\n",
    "    \"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"MONEY\",\"NORP\",\"EVENT\",\"PRODUCT\"\n",
    "]\n",
    "\n",
    "def extract_important_entities(text: str, labels=None, top_k=None):\n",
    "    \"\"\"\n",
    "    Extract named entities and return a structured result with simple importance scoring.\n",
    "    - labels: filter to these labels (defaults to IMPORTANT_LABELS)\n",
    "    - top_k: if set, return only the top_k entities by importance\n",
    "    Importance score = frequency + 0.25*token_length (simple heuristic).\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        labels = IMPORTANT_LABELS\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # collect entities\n",
    "    ents = [(ent.text.strip(), ent.label_) for ent in doc.ents if ent.label_ in labels]\n",
    "\n",
    "    freq = Counter([e[0] for e in ents])\n",
    "    label_map = defaultdict(lambda: None)\n",
    "    length_map = {}\n",
    "\n",
    "    for ent_text, ent_label in ents:\n",
    "        label_map[ent_text] = ent_label \n",
    "        length_map[ent_text] = len(ent_text.split())\n",
    "\n",
    "    # score: freq + 0.25 * length (nicer names bubble up)\n",
    "    scored = []\n",
    "    for ent_text, f in freq.items():\n",
    "        score = f + 0.25 * length_map[ent_text]\n",
    "        scored.append({\n",
    "            \"text\": ent_text,\n",
    "            \"label\": label_map[ent_text],\n",
    "            \"frequency\": f,\n",
    "            \"length_tokens\": length_map[ent_text],\n",
    "            \"importance\": round(score, 3),\n",
    "        })\n",
    "\n",
    "    # sort by importance desc, then frequency desc, then text\n",
    "    scored.sort(key=lambda x: (-x[\"importance\"], -x[\"frequency\"], x[\"text\"].lower()))\n",
    "\n",
    "    if top_k is not None:\n",
    "        scored = scored[:top_k]\n",
    "\n",
    "    # also group by label for convenience\n",
    "    by_label = defaultdict(list)\n",
    "    for item in scored:\n",
    "        by_label[item[\"label\"]].append(item[\"text\"])\n",
    "\n",
    "    return {\n",
    "        \"entities_ranked\": scored,   # list of dicts with scores\n",
    "        \"entities_by_label\": dict(by_label)  # quick lookup\n",
    "    }\n",
    "\n",
    "# example\n",
    "text = \"\"\"\n",
    "Apple announced a $2 billion investment in Austin, Texas on September 21, 2025.\n",
    "Tim Cook met with officials from the U.S. Department of Commerce.\n",
    "The iPhone 16 Pro launch is expected in October.\n",
    "\"\"\"\n",
    "\n",
    "res = extract_important_entities(text, top_k=10)\n",
    "for e in res[\"entities_ranked\"]:\n",
    "    print(e)\n",
    "print(\"\\nBy label:\", res[\"entities_by_label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f96f3fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>categories</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>noun_list</th>\n",
       "      <th>adj_list</th>\n",
       "      <th>entities_ranked</th>\n",
       "      <th>entities_by_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/14826</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
       "      <td>[trade]</td>\n",
       "      <td>asian exporters fear damage from u.s.-japan ri...</td>\n",
       "      <td>[exporters, damage, rift, trade, friction, u.s...</td>\n",
       "      <td>[asian, u.s.-japan, many, far-reaching, econom...</td>\n",
       "      <td>[{'text': 'u.s.', 'label': 'GPE', 'frequency':...</td>\n",
       "      <td>{'GPE': ['u.s.', 'japan', 'taiwan', 'hong kong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/14828</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
       "      <td>[grain]</td>\n",
       "      <td>china daily says vermin eat 7-12 pct grain sto...</td>\n",
       "      <td>[china, eat, pct, grain, stocks, survey, provi...</td>\n",
       "      <td>[vermin, 7-12, vermin, china, inadequate, bad,...</td>\n",
       "      <td>[{'text': 'china', 'label': 'GPE', 'frequency'...</td>\n",
       "      <td>{'GPE': ['china'], 'ORG': ['the china daily', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/14829</td>\n",
       "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
       "      <td>[crude, nat-gas]</td>\n",
       "      <td>japan to revise long-term energy demand downwa...</td>\n",
       "      <td>[japan, energy, demand, ministry, trade, indus...</td>\n",
       "      <td>[long-term, international, long-term, japanese...</td>\n",
       "      <td>[{'text': 'miti', 'label': 'ORG', 'frequency':...</td>\n",
       "      <td>{'ORG': ['miti', 'the ministry of internationa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/14832</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
       "      <td>[corn, grain, rice, rubber, sugar, tin, trade]</td>\n",
       "      <td>thai trade deficit widens in first quarter tha...</td>\n",
       "      <td>[thai, trade, deficit, quarter, thailand, trad...</td>\n",
       "      <td>[first, first, improved, pct, raw, semi-finish...</td>\n",
       "      <td>[{'text': 'first quarter', 'label': 'DATE', 'f...</td>\n",
       "      <td>{'DATE': ['first quarter', 'the first quarter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/14833</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...</td>\n",
       "      <td>[palm-oil, veg-oil]</td>\n",
       "      <td>indonesia sees cpo price rising sharply indone...</td>\n",
       "      <td>[indonesia, sees, price, oil, cpo, prices, dlr...</td>\n",
       "      <td>[cpo, indonesia, crude, palm, better, european...</td>\n",
       "      <td>[{'text': 'indonesia', 'label': 'GPE', 'freque...</td>\n",
       "      <td>{'GPE': ['indonesia', 'malaysia'], 'PERSON': [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_id                                               text  \\\n",
       "0  test/14826  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...   \n",
       "1  test/14828  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...   \n",
       "2  test/14829  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...   \n",
       "3  test/14832  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...   \n",
       "4  test/14833  INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...   \n",
       "\n",
       "                                       categories  \\\n",
       "0                                         [trade]   \n",
       "1                                         [grain]   \n",
       "2                                [crude, nat-gas]   \n",
       "3  [corn, grain, rice, rubber, sugar, tin, trade]   \n",
       "4                             [palm-oil, veg-oil]   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  asian exporters fear damage from u.s.-japan ri...   \n",
       "1  china daily says vermin eat 7-12 pct grain sto...   \n",
       "2  japan to revise long-term energy demand downwa...   \n",
       "3  thai trade deficit widens in first quarter tha...   \n",
       "4  indonesia sees cpo price rising sharply indone...   \n",
       "\n",
       "                                           noun_list  \\\n",
       "0  [exporters, damage, rift, trade, friction, u.s...   \n",
       "1  [china, eat, pct, grain, stocks, survey, provi...   \n",
       "2  [japan, energy, demand, ministry, trade, indus...   \n",
       "3  [thai, trade, deficit, quarter, thailand, trad...   \n",
       "4  [indonesia, sees, price, oil, cpo, prices, dlr...   \n",
       "\n",
       "                                            adj_list  \\\n",
       "0  [asian, u.s.-japan, many, far-reaching, econom...   \n",
       "1  [vermin, 7-12, vermin, china, inadequate, bad,...   \n",
       "2  [long-term, international, long-term, japanese...   \n",
       "3  [first, first, improved, pct, raw, semi-finish...   \n",
       "4  [cpo, indonesia, crude, palm, better, european...   \n",
       "\n",
       "                                     entities_ranked  \\\n",
       "0  [{'text': 'u.s.', 'label': 'GPE', 'frequency':...   \n",
       "1  [{'text': 'china', 'label': 'GPE', 'frequency'...   \n",
       "2  [{'text': 'miti', 'label': 'ORG', 'frequency':...   \n",
       "3  [{'text': 'first quarter', 'label': 'DATE', 'f...   \n",
       "4  [{'text': 'indonesia', 'label': 'GPE', 'freque...   \n",
       "\n",
       "                                   entities_by_label  \n",
       "0  {'GPE': ['u.s.', 'japan', 'taiwan', 'hong kong...  \n",
       "1  {'GPE': ['china'], 'ORG': ['the china daily', ...  \n",
       "2  {'ORG': ['miti', 'the ministry of internationa...  \n",
       "3  {'DATE': ['first quarter', 'the first quarter ...  \n",
       "4  {'GPE': ['indonesia', 'malaysia'], 'PERSON': [...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_ner_columns(df: pd.DataFrame, text_col: str, top_k=10):\n",
    "    def _extract(row_text):\n",
    "        out = extract_important_entities(row_text, top_k=top_k)\n",
    "        return {\n",
    "            \"entities_ranked\": out[\"entities_ranked\"],\n",
    "            \"entities_by_label\": out[\"entities_by_label\"]\n",
    "        }\n",
    "    tmp = df[text_col].apply(_extract)\n",
    "    df[\"entities_ranked\"] = tmp.apply(lambda d: d[\"entities_ranked\"])\n",
    "    df[\"entities_by_label\"] = tmp.apply(lambda d: d[\"entities_by_label\"])\n",
    "    return df\n",
    "\n",
    "reuters_df = add_ner_columns(reuters_df, \"cleaned_text\", 10)\n",
    "\n",
    "reuters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8732b86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asian exporters fear damage from u.s.-japan rift mounting trade friction between the u.s. and japan has raised fears among many of asia\\'s exporting nations that the row could inflict far-reaching economic damage, businessmen and officials said. they told reuter correspondents in asian capitals a u.s. move against japan might boost protectionist sentiment in the u.s. and lead to curbs on american imports of their products. but some exporters said that while the conflict would hurt them in the long-run, in the short-term tokyo\\'s loss might be their gain. the u.s. has said it will impose 300 mln dlrs of tariffs on imports of japanese electronics goods on april 17, in retaliation for japan\\'s alleged failure to stick to a pact not to sell semiconductors on world markets at below cost. unofficial japanese estimates put the impact of the tariffs at 10 billion dlrs and spokesmen for major electronics firms said they would virtually halt exports of products hit by the new taxes. \"we wouldn\\'t be able to do business,\" said a spokesman for leading japanese electronics firm matsushita electric industrial co ltd &lt;mc.t>. \"if the tariffs remain in place for any length of time beyond a few months it will mean the complete erosion of exports (of goods subject to tariffs) to the u.s.,\" said tom murtha, a stock analyst at the tokyo office of broker &lt;james capel and co>. in taiwan, businessmen and officials are also worried. \"we are aware of the seriousness of the u.s. threat against japan because it serves as a warning to us,\" said a senior taiwanese trade official who asked not to be named. taiwan had a trade trade surplus of 15.6 billion dlrs last year, 95 pct of it with the u.s. the surplus helped swell taiwan\\'s foreign exchange reserves to 53 billion dlrs, among the world\\'s largest. \"we must quickly open our markets, remove trade barriers and cut import tariffs to allow imports of u.s. products, if we want to defuse problems from possible u.s. retaliation,\" said paul sheen, chairman of textile exporters &lt;taiwan safe group>. a senior official of south korea\\'s trade promotion association said the trade dispute between the u.s. and japan might also lead to pressure on south korea, whose chief exports are similar to those of japan. last year south korea had a trade surplus of 7.1 billion dlrs with the u.s., up from 4.9 billion dlrs in 1985. in malaysia, trade officers and businessmen said tough curbs against japan might allow hard-hit producers of semiconductors in third countries to expand their sales to the u.s. in hong kong, where newspapers have alleged japan has been selling below-cost semiconductors, some electronics manufacturers share that view. but other businessmen said such a short-term commercial advantage would be outweighed by further u.s. pressure to block imports. \"that is a very short-term view,\" said lawrence mills, director-general of the federation of hong kong industry. \"if the whole purpose is to prevent imports, one day it will be extended to other sources. much more serious for hong kong is the disadvantage of action restraining trade,\" he said. the u.s. last year was hong kong\\'s biggest export market, accounting for over 30 pct of domestically produced exports. the australian government is awaiting the outcome of trade talks between the u.s. and japan with interest and concern, industry minister john button said in canberra last friday. \"this kind of deterioration in trade relations between two countries which are major trading partners of ours is a very serious matter,\" button said. he said australia\\'s concerns centred on coal and beef, australia\\'s two largest exports to japan and also significant u.s. exports to that country. meanwhile u.s.-japanese diplomatic manoeuvres to solve the trade stand-off continue. japan\\'s ruling liberal democratic party yesterday outlined a package of economic measures to boost the japanese economy. the measures proposed include a large supplementary budget and record public works spending in the first half of the financial year. they also call for stepped-up spending as an emergency measure to stimulate the economy despite prime minister yasuhiro nakasone\\'s avowed fiscal reform program. deputy u.s. trade representative michael smith and makoto kuroda, japan\\'s deputy minister of international trade and industry (miti), are due to meet in washington this week in an effort to end the dispute.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df.iloc[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70bb52b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exporters', 'damage', 'rift', 'trade', 'friction']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df.iloc[0,4][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deebc5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asian', 'u.s.-japan', 'many', 'far-reaching', 'economic']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df.iloc[0,5][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f8e8022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'u.s.',\n",
       "  'label': 'GPE',\n",
       "  'frequency': 17,\n",
       "  'length_tokens': 1,\n",
       "  'importance': 17.25},\n",
       " {'text': 'japan',\n",
       "  'label': 'GPE',\n",
       "  'frequency': 12,\n",
       "  'length_tokens': 1,\n",
       "  'importance': 12.25},\n",
       " {'text': 'japanese',\n",
       "  'label': 'NORP',\n",
       "  'frequency': 4,\n",
       "  'length_tokens': 1,\n",
       "  'importance': 4.25},\n",
       " {'text': 'last year',\n",
       "  'label': 'DATE',\n",
       "  'frequency': 3,\n",
       "  'length_tokens': 2,\n",
       "  'importance': 3.5},\n",
       " {'text': 'taiwan',\n",
       "  'label': 'GPE',\n",
       "  'frequency': 3,\n",
       "  'length_tokens': 1,\n",
       "  'importance': 3.25}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df.iloc[0,6][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a286c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GPE': ['u.s.', 'japan', 'taiwan', 'hong kong', 'south korea'],\n",
       " 'NORP': ['japanese', 'asian'],\n",
       " 'DATE': ['last year', 'the first half of the financial year'],\n",
       " 'ORG': ['matsushita electric industrial co ltd &lt;mc.t>']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df.iloc[0,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35892618",
   "metadata": {},
   "source": [
    "# Transformer-based few-shot extraction (Experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72afdc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"person\": \"Tim Cook\", \"organization\": \"Apple\", \"date\": \"Sept 21, 2025\", \"city\": \"Austin\", \"amount\": \"$2 billion\" ###\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_id = \"google/flan-t5-base\"  # light llm\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "mdl = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "def generate(prompt, max_new_tokens=256):\n",
    "    x = tok(prompt, return_tensors=\"pt\")\n",
    "    y = mdl.generate(**x, max_new_tokens=max_new_tokens)\n",
    "    return tok.decode(y[0], skip_special_tokens=True)\n",
    "\n",
    "few_shot_prompt = \"\"\"You extract entities and return strict JSON with keys:\n",
    "{\"person\": str|null, \"organization\": str|null, \"date\": str|null, \"city\": str|null, \"amount\": str|null}\n",
    "\n",
    "### Example 1\n",
    "Text: \"Google will invest $500 million in Dublin on 3 July 2024, Sundar Pichai said.\"\n",
    "JSON: {\"person\": \"Sundar Pichai\", \"organization\": \"Google\", \"date\": \"3 July 2024\", \"city\": \"Dublin\", \"amount\": \"$500 million\"}\n",
    "\n",
    "### Example 2\n",
    "Text: \"On Sept 21, 2025, Apple opened a new office in Austin.\"\n",
    "JSON: {\"person\": null, \"organization\": \"Apple\", \"date\": \"Sept 21, 2025\", \"city\": \"Austin\", \"amount\": null}\n",
    "\n",
    "### Task\n",
    "Text: \"Tim Cook confirmed Apple’s $2 billion project in Austin, Texas on September 21, 2025.\"\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "print(generate(few_shot_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8486e0",
   "metadata": {},
   "source": [
    "# tf-idf-based summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4263fbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tim Cook met with officials from the U.S. Department of Commerce. The project will expand manufacturing and create 2,000 jobs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extractive_summary(text, top_n=2):\n",
    "    # split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # compute tf-idf on sentences\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # sentence scores = sum of tf-idf weights\n",
    "    scores = tfidf_matrix.sum(axis=1).A.ravel()\n",
    "\n",
    "    # pick top_n sentences\n",
    "    top_idx = np.argsort(scores)[-top_n:]\n",
    "    summary = [sentences[i] for i in sorted(top_idx)]\n",
    "    return \" \".join(summary)\n",
    "\n",
    "doc = \"\"\"Apple announced a $2 billion investment in Austin, Texas. \n",
    "Tim Cook met with officials from the U.S. Department of Commerce. \n",
    "The project will expand manufacturing and create 2,000 jobs. \n",
    "Apple is competing with Samsung and Google in AI chips.\"\"\"\n",
    "print(extractive_summary(doc, top_n=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155ad0af",
   "metadata": {},
   "source": [
    "# abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02f8f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Your max_length is set to 50, but your input_length is only 48. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "/Users/sm/Documents/GitHub/agentic_text_analyzer/.venv/lib/python3.12/site-packages/transformers/pytorch_utils.py:339: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple announced a $2 billion investment in Austin, Texas on September 21, 2025. The project will expand local manufacturing and create 2,000 jobs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "text = \"\"\"Apple announced a $2 billion investment in Austin, Texas on September 21, 2025. \n",
    "Tim Cook met with officials to discuss workforce programs and incentives. \n",
    "The project will expand local manufacturing and create 2,000 jobs.\"\"\"\n",
    "\n",
    "summary = summarizer(text, max_length=50, min_length=20, do_sample=False)[0]['summary_text']\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e30c33",
   "metadata": {},
   "source": [
    "# Evals on Summarization Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c45d8700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Eval dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:10]\")  # sample 10\n",
    "# metric\n",
    "rouge = evaluate.load(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3778437c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.2300424167041934), 'rouge2': np.float64(0.05107991780487174), 'rougeL': np.float64(0.15006118398979001), 'rougeLsum': np.float64(0.18343688548824633)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds = [extractive_summary(x[\"article\"], top_n=2) for x in dataset]\n",
    "refs  = [x[\"highlights\"] for x in dataset]\n",
    "\n",
    "results = rouge.compute(predictions=preds, references=refs)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d0d484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.4190002940814237), 'rouge2': np.float64(0.2154233119778794), 'rougeL': np.float64(0.3251416255254694), 'rougeLsum': np.float64(0.3659612867530163)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds = [summarizer(x[\"article\"], max_length=50, min_length=20, do_sample=False)[0]['summary_text'] for x in dataset]\n",
    "refs  = [x[\"highlights\"] for x in dataset]\n",
    "\n",
    "results = rouge.compute(predictions=preds, references=refs)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20e41c",
   "metadata": {},
   "source": [
    "## Much better results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbb085",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
